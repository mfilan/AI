{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "For this task we'll consider the problem of predicting the price of a house in Boston. The following code loads the data (`load_boston`) and represents them as `torch.Tensor`, a PyTorch-specific representation of a tensor (a tensor is a generalization of a matrix to more than 2 dimensions). The `dtype` parameter forces conversion to floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = torch.tensor(boston['data'], dtype=torch.float)\n",
    "y = torch.tensor(boston['target'], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the data randomly into three subsets: a training set consisting of 70\\% of the learning examples, a validation set consisting of 10\\% and a test set consisting of the remaining 20\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = X.shape\n",
    "n_train = int(.7*n)\n",
    "n_validation = int(.1*n)\n",
    "indices = np.random.permutation(n)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "X_train = X[:n_train, :]\n",
    "y_train = y[:n_train]\n",
    "X_validation = X[n_train:n_train+n_validation, :]\n",
    "y_validation = y[n_train:n_train+n_validation]\n",
    "X_test = X[n_train+n_validation:, :]\n",
    "y_test = y[n_train+n_validation:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple regressor\n",
    "\n",
    "PyTorch is based on modules (objects of the class `torch.nn.Module`), which are composed into a compution graph. Each module may use parameters (objects of the class `torch.nn.Parameter`), and for each such an object it is possible to automatically compute gradients and optimize them according to some cost function.\n",
    "\n",
    "We begin by constructing a single linear layer, that is, a layer implementing the operation $\\hat{y} = Xw + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = nn.Linear(p, 1) # p input features, 1 output feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance we use mean-squared error. While PyTorch offers a ready to use implementation as the class `torch.nn.MSELoss()`, this time we implement it manually as an example. The main point is to inherit from the class `torch.nn.Module` and then to override the method `forward`, which is responsible for performing computations in the forward direction of the computation graph. Computations in the backward direction (that is, gradient flow) are derrived automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSE, self).__init__()\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        return ((prediction-target)**2).mean()\n",
    "\n",
    "mse = MSE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to optimize the vector `w` and `b` automatically during the learning. To this end, we use the class `torch.optim.Adam`, which implements some extensions of the graident descent algorithm. We construct the object `opt`, which is responsible for optimizing the regressor's parameters, obtained by calling `regressor.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(regressor.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting computations\n",
    "\n",
    "We implement mini-batch learning over `n_epoch` epochs. Within each epoch:\n",
    "1. The indices of the training set are shuffled.\n",
    "2. We iterate over these indices, retrieving `batch_size` of them every time. We use these indices to select examples used in the current learning step.\n",
    "3. We zero the gradients stored by the optimizer (`opt.zero_grad()`)\n",
    "4. We execute the regressor and compute the mean-squared error (MSE).\n",
    "5. We compute the gradients (`mse_value.backward()`) and apply them (`opt.step()`) to update the parameters of the regressor\n",
    "6. We store the value of MSE. We call the `detach()` method to detach the MSE and its gradients.\n",
    "\n",
    "After an epoch is completed, we average the collected MSEs and store the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values = []\n",
    "batch_size = 100\n",
    "n_epoch = 1000\n",
    "for epoch in range(n_epoch):\n",
    "    indices = np.random.permutation(n_train)\n",
    "    mse_epoch = []\n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        end = min(start + batch_size, len(indices))\n",
    "        indices_batch = indices[start:end]\n",
    "        X_batch = X_train[indices_batch, :]\n",
    "        y_batch = y_train[indices_batch]\n",
    "        opt.zero_grad()\n",
    "        y_pred = regressor(X_batch).reshape((-1,))\n",
    "        mse_value = mse(y_pred, y_batch)\n",
    "        mse_value.backward()\n",
    "        opt.step()\n",
    "        mse_epoch.append(mse_value.detach())\n",
    "    mse_values.append(np.mean(mse_epoch))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the two preceeding cells of the code again. Because the regressor and the optimizer were not created anew, so the parameters are already optimized and the obtained charts are vastly different. In order to return to the initial state it is necessary to create new obiect `regressor` and `opt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement early stopping: after each epoch of learning compute the value of MSE using the sets `X_validation` i `y_validation`. Observe that we must call the methods\n",
    "`regressor.train()` and `regressor.eval()` to switch the module between the training mode and the evaluation mode. Remeber not to call `opt.step()` after performing the computation on the validation set. Stop the learning process if there is no improvement over the last 100 epochs with regard to the value of MSE on the validation set. Store values of `mse` for training and for validation (respecitvely in `training_mses` and `validation_mses`) and plot them. Print out the number of the epoch when the learning process was stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = nn.Linear(p, 1) # p input features, 1 output feature\n",
    "opt = optim.Adam(regressor.parameters())\n",
    "\n",
    "training_mses = []\n",
    "validation_mses = []\n",
    "batch_size = 100\n",
    "n_epoch = 10000\n",
    "for epoch in range(n_epoch):\n",
    "    regressor.train()\n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        ...\n",
    "    regressor.eval()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_mses, 'b') # plot training errors in blue\n",
    "plt.plot(validation_mses, 'r') # plot validation errors in red\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the L1 regularization following the equation below by implementing the method `forward` in the class `MSEWithL1`\n",
    "\n",
    "$$ cost = MSE + \\alpha \\sum_{i=1}^n \\left|w_i\\right| $$\n",
    "\n",
    "Observe that the sum starts in 1, not in 0 - this is because the intercept is not included in this sum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEWithL1(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha, weight):\n",
    "        super(MSEWithL1, self).__init__()\n",
    "        self._weight = weight\n",
    "        self._alpha = alpha\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your solution: train for 5000 epochs for each of the following values of $\\alpha$: 0.01, 0.1, 1, 10, 100. Every time create the regressor and the optimizer from scratch. Every epoch, collect MSE (not the total cost defined by `MSEWithL1`) on the validation set and plot them on a single chart for all tested values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSE()\n",
    "\n",
    "mses = {}\n",
    "for alpha in [0.01, 0.1, 1, 10, 100]:    \n",
    "    regressor = ...\n",
    "    opt = ...\n",
    "    cost = MSEWithL1(alpha, regressor.weight)\n",
    "    mses[alpha] = [] # store the MSE values on the validation set here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in sorted(mses.keys()):\n",
    "    plt.plot(mses[alpha])\n",
    "plt.legend([str(alpha) for alpha in sorted(mses.keys())])\n",
    "plt.show()\n",
    "for alpha in sorted(mses.keys()):\n",
    "    plt.plot(mses[alpha][200:])\n",
    "plt.legend([str(alpha) for alpha in sorted(mses.keys())])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which of the values of $\\alpha$ is the best?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part is concerned with the classification task. We begin by downloading the MNIST dataset, consisting of 70.000 examples of handwritten digits (0-9), represented as grayscale images of 28x28 pixels. We use the function `fetch_openml` that downloads the data from [https://www.openml.org/](https://www.openml.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(mnist.data, dtype=torch.float)\n",
    "y = torch.tensor([int(v) for v in mnist.target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code splits the data into three subsets: training, validation and test. This time, instead of representing the sets as raw tensors, we wrap each of them into an object of the class `torch.utils.data.TensorDataset`, which then enables shuffling, grouping into mini-batches and iterating over using `torch.utils.data.DatasetLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = mnist.data.shape\n",
    "k = 10 # liczba klas\n",
    "n_train = int(.7*n)\n",
    "n_validation = int(.1*n)\n",
    "indices = np.random.permutation(n)\n",
    "train_indices = indices[:n_train]\n",
    "validation_indices = indices[n_train:n_train+n_validation]\n",
    "test_indices = indices[n_train+n_validation:]\n",
    "\n",
    "ds_train = data.TensorDataset(X[train_indices,:], y[train_indices])\n",
    "ds_validation = data.TensorDataset(X[validation_indices,:], y[validation_indices])\n",
    "ds_test = data.TensorDataset(X[test_indices,:], y[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple logisitc regression model, learning on the raw pixels of the images. The matrix of the feature weights (here: pixels) is of the size $p \\times k$, i.e., $p$ weights for each of the $k$ classes. The model computes logits, i.e., the output of the logistic  regression **before** applying the softmax function. In the output, each row corresponds to a single example and each column to a single class. Should each row be normalized using the softmax function, each row would be a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(p, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the loss function we will use *cross entropy* and perform the optimization using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an auxiliary function to compute the classification accuracy. For each row of the `logits` matrix we select the number of the column with the highest value and compare it with the expected (true) value. This way we obtain a vector of 0s and 1s, which are then averaged to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, expected):\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return (pred == expected).type(torch.float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder is very similar to the linear regression: during each of the `n_epoch` epochs, we enable the training mode (`model.train()`), create a data loder ( `torch.utils.data.DataLoader`) to handle shuffling and creating batches for us and then iterate over the loader, computing loss and gradients. Then we evaluate by computing the accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "acc_values = []\n",
    "batch_size = 100\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    model.train()\n",
    "    loader = data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)    \n",
    "    epoch_loss = []\n",
    "    for X_batch, y_batch in loader:\n",
    "        opt.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = cost(logits, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()        \n",
    "        epoch_loss.append(loss.detach())\n",
    "    model.eval()\n",
    "    loss_values.append(torch.tensor(epoch_loss).mean())\n",
    "    logits = model(ds_validation.tensors[0])\n",
    "    acc = compute_acc(logits, ds_validation.tensors[1])\n",
    "    acc_values.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss on the training set\")\n",
    "plt.plot(loss_values)\n",
    "plt.show()\n",
    "plt.title(\"Accuracy on the validation set\")\n",
    "plt.plot(acc_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and modify the code presented above so that it uses early stopping instead of training over a constant number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, possibly in more cells than one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: A neural network with a hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression is a neural network without hidden layers. Use the code presented above and extend it with a hidden layer of 500 neurons. In order to connect a few objects of the type `torch.nn.Module` into a sequence use the class `torch.nn.Sequence`. Use the leaky ReLU function (`torch.nn.LeakyReLU`) to implement non-linearity. \n",
    "\n",
    "Implement early stopping. During the training collect the loss and accuracy on the  training set and the accuracy on the validation set and plot them. If the training takes too much time, modify `batch_size` and/or the number of neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, possibly in more cells than one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
